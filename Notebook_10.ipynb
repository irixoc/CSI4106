{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook-10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsCQnroFWbpA"
      },
      "source": [
        "# Notebook 10 - Reinforcement Learning / Self-Driving Cab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS9f1M7hWblx"
      },
      "source": [
        "CSI4106 Artificial Intelligence   \n",
        "Fall 2021  \n",
        "Version 1 (2020) prepared by Julian Templeton and Caroline Barrière.  Version 2 (2021) adapted by Caroline Barrière."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWknU1E2Wbi8"
      },
      "source": [
        "***INTRODUCTION***:  \n",
        "In this notebook we will be exploring the use of Reinforcment Learning to help allow an agent solve a specific task in an environment provided by [OpenAI's Gym library](https://gym.openai.com/). OpenAI's Gym provides many different experiments to use. These range from balancing acts to self driving cars to playing a simple Atari game. Unfortunately not every option available to us can be easily worked with. Many can take hours of training to start seeing some exciting results. Each of these experiments use agents that can be trained by Reinforcment Learning to master how to perform the specified task. The methods used can range from the simple use of Q-Learning to the more complex use of one or more Deep Learning models that work in conjunction with Reinforcement Learning techniques. \n",
        "\n",
        "Within this notebook we will be exploring a scenario in which an autonomous taxi located on a grid must pickup a passenger located in one of four positions and drop the passenger off in one of three other positions.    \n",
        "\n",
        "To familiarize yourself with the **Self-Driving Cab problem** tackled in this notebook, please go to the site https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/ and read:  \\\n",
        "\n",
        "*   section 1 (rewards)\n",
        "*   section 2 (state space) which will make you understand why there are 500 possible states\n",
        "*   section 3 (action space) which describes the possible actions.  \n",
        "\n",
        "Throughout the notebook we will be working with a random baseline approach and a Q-Learning-based approach. This will provide insight into how Q-Learning can be applied to problems and how an agent can use Reinforcment Learning to solve problems in an environment.  Comparison with the baseline approach will show the potential of Q-Learning.  \n",
        "\n",
        "**When submitting this notebook, ensure that you do NOT reset the outputs from running the code (plus remember to save the notebook with ctrl+s).**      \n",
        "\n",
        "**In order to keep the installation easy, you will be once again running this notebook in Google Colab, NOT on your local machine.**    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iCMGO9VWbgM"
      },
      "source": [
        "***HOMEWORK***:  \n",
        "Go through the notebook by running each cell, one at a time.  \n",
        "Look for **(TO DO)** for the tasks that you need to perform. Do not edit the code outside of the questions which you are asked to answer unless specifically asked. Once you're done, sign the notebook (at the end of the notebook), rename it to *StudentNumber-LastName-Notebook10.ipynb* and submit it.  \n",
        "\n",
        "*The notebook will be marked on 30.  \n",
        "Each **(TO DO)** has a number of points associated with it.*\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIhGNTB-Wbbs"
      },
      "source": [
        "**1.0 - Setting up the Taxi Game**   \n",
        "\n",
        "To begin the notebook, we need to set up and explore the Open Gym autonomous taxi environment.  \n",
        "\n",
        "The autonomous taxi will pick up and dropoff a passenger within a small grid. To really understand the environment in which the autonomous taxi (the agent) evolves, make sure you read the 3 sections about rewards, state space and action space from the tutorial site, as mentioned in the introduction of this notebook.\n",
        "\n",
        "The code used throughout the notebook comes from [this example](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/) and has been modified accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEQK-Xep0MrF"
      },
      "source": [
        "To start, we will install some of the packages that we will need to run the progam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPYIG4d6Ztzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ea708e-d03e-47ab-b715-cbc774f0fb33"
      },
      "source": [
        "# Install the necessary libraries\n",
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0vOVuqHA8SM"
      },
      "source": [
        "# Import the necessary libraries\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3nhPCUHb2cO"
      },
      "source": [
        "With all of the libraries installed, we will now make use of the Taxi program provided by Gym. Below we will import Gym, load the program as the active environment, and render an image representing the current state of the program.   \n",
        "\n",
        "From the image seen below, there are four different key locations in the environment, represented by *R*, *G*, *Y*, and *B*. The letter that is *bolded in blue* represents where the current passenger needs to get picked up and the letter *bolded in purple* represents where the passenger wants to dropped off. The *yellow block* represents the cell which the taxi cab is currently located at. Therefore, the taxi cab must first pick up the passenger and drop them off at the dropoff location. When a passenger is in the taxi, the yellow block turns to a *green block* until the passenger is dropped off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2iMY2MbaFSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad152d88-a3de-4dab-ae70-91f9d510602b"
      },
      "source": [
        "# Load the environment\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "# Render the current state of the program\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oq2BvrwdPh-"
      },
      "source": [
        "Next we will reset the state of the environment and re-render the current state. We also print the total number of actions available to our agent (defined as the *Action Space*) and the *State Space* which represents the state of the program (where is the cab, the passenger, pickup location and dropoff location)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlLevJ7KaHYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09003234-0729-4dd2-966d-885c08a9b3dd"
      },
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGp8quCqeGjU"
      },
      "source": [
        "We want our agent to learn which action to take given a specific state. This state depends on where the taxi cab is located in relation to the passenger location and drop off location. The six possible actions that the taxi can take at a given time step are:    \n",
        "\n",
        "Action = 0: Head south    \n",
        "Action = 1: Head north    \n",
        "Action = 2: Head east    \n",
        "Action = 3: Head west    \n",
        "Action = 4: Pickup     \n",
        "Action = 5: Dropoff    \n",
        "\n",
        "Below is an example of setting the state to a specific encoding and rendering that state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7QM71s0aded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082852e0-33d5-4e84-ae69-0ef9f04f91ad"
      },
      "source": [
        "# The encoding below represents: (taxi row, taxi column, passenger location, destination index)\n",
        "state = env.encode(3, 1, 2, 0) \n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCSHJMfb2Hll"
      },
      "source": [
        "The following example showcases how a state with the passenger within the taxi can be set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPshGWDomKfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7d358f-94d9-49c8-cd12-f45ec6c0c74a"
      },
      "source": [
        "# The encoding below represents: (taxi row, taxi column, passenger location, destination index)\n",
        "state = env.encode(0, 1, 4, 0) \n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 36\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3b__QWRTRB3"
      },
      "source": [
        "**(TO DO) Q1 - 4 marks**    \n",
        "Now that we have seen how to set a state via an encoding, you will need to set the state to match two different descriptions below and render them.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YKbEIP2WAkb"
      },
      "source": [
        "**(TO DO) Q1 (a) - 2 marks**    \n",
        "Set the passenger to be at position G, with the passenger wanting to be dropped off at position R, and the taxi positioned at a random point on the grid (the selected position of the taxi must be selected randomly). After setting the position, render the state.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugCARMZY3qoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c690439-a0b7-4c96-e152-46e99420d036"
      },
      "source": [
        "# ANSWER Q1(a) \n",
        "# remember to use random coordinates within the grid for the taxi...\n",
        "import random\n",
        "taxi_r = random.randint(0,4) # random integer for row of taxi\n",
        "taxi_c = random.randint(0,4) # random integer for column of taxi\n",
        "\n",
        "state_1a = env.encode(taxi_r, taxi_c, 1, 0) \n",
        "print(\"State:\", state_1a)\n",
        "\n",
        "env.s = state_1a\n",
        "env.render()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 264\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pl8bVtwWIdE"
      },
      "source": [
        "**(TO DO) Q1 (b) - 2 marks**    \n",
        "Set the passenger to be in the taxi (at any position without a letter on it) and set the passenger dropoff point to be position B. After setting the position, render the state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0rpk0D830DO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a716888-4403-4dc0-b8af-6b7870ea4ae5"
      },
      "source": [
        "# ANSWER Q1(b)\n",
        "# Set passenger to be in the taxi at any position without a letter on it, and set drop off point to be position B\n",
        "state_1b = env.encode(3, 3, 4, 3) \n",
        "print(\"State:\", state_1b)\n",
        "\n",
        "env.s = state_1b\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 379\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTyOS_Hbi3Rf"
      },
      "source": [
        "For every action that the taxi can take, we have a list representing the key information with respect to what will happen when an action is performed. After performing an action, the agent will receive a reward or a penalty from the environment which will then be in a different state.\n",
        "\n",
        "Below we display a dictionary (very similar to the Reward Table we discussed in the video lecture) that contains all possible actions along with the following information within the corresponding tuples:     \n",
        "\n",
        "(     \n",
        "  The probability of taking that action,     \n",
        "  The resulting state after taking that action,    \n",
        "  The reward for taking that action,    \n",
        "  Whether or not the program will end when performing the action   \n",
        ")      \n",
        "\n",
        "Example tuple: (1.0, 328, -1, False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Ga3Y6yagfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282d9111-f8c2-4ad1-c578-33cf0dc0e7db"
      },
      "source": [
        "env.P[328]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_57Z2adUzAJH"
      },
      "source": [
        "Although not displayed by the code above, if the taxi is holding the passenger and is over the dropoff point, the reward for the dropoff action is 20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iiy-KNusvBDJ"
      },
      "source": [
        "**2.0 - Baseline Approach to the Taxi Game**   \n",
        "\n",
        "To start, we will perform the simulation of the taxi cab scenario with a baseline approach that does not use Q-Learning. This approach will simply work by selecting a random available action at each time step, regardless of the current state. We will also prepare a method of playing through all frames within an episode to view how the agent controls the taxi in the scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKnHAA5SaitM"
      },
      "source": [
        "def run_single_simulation_baseline(env, state, disable_prints=False):\n",
        "    '''\n",
        "    Given the environment and a specific state, randomly select an action for the taxi\n",
        "    to perform until the goal is completed.\n",
        "    '''\n",
        "    if not disable_prints:\n",
        "        print(\"Testing for simulation: {}\".format(state))\n",
        "    # Set the state of the environment\n",
        "    env.s = state\n",
        "    # Used to hold all information for a single time step (including the image data)\n",
        "    frames = []\n",
        "    # Used to determine when the simulation has been completed\n",
        "    done = False\n",
        "    # Determines the number of times steps that the application has been run for\n",
        "    time_steps = 0\n",
        "    # The total values used to determine how many times the agent mistakenly\n",
        "    # picks up no one or attempts to dropoff no passenger or attempts to\n",
        "    # dropoff a passenger in the wrong position.\n",
        "    penalties, reward = 0, 0\n",
        "    # Run until the passenger has been picked up and dropped off in the target location\n",
        "    while not done:\n",
        "        # Perform a random action from the set of available actions in the environment\n",
        "        action = env.action_space.sample()\n",
        "        # From performing the action, retrieve the new state, the reward from taking the action,\n",
        "        # whether the simulation is complete, and other information from performing the action.\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # If an incorrect dropoff or pickup is performed, increment the penalty count\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "        # Put each rendered frame into dict to use for animating the process and\n",
        "        # tracking the details over the run\n",
        "        frames.append({\n",
        "            'frame': env.render(mode='ansi'),\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward\n",
        "            }\n",
        "        )\n",
        "        # Increment the time step count\n",
        "        time_steps += 1\n",
        "    # State the total number of steps taken and the total penalties that have occured.\n",
        "    if not disable_prints:\n",
        "        print(\"Timesteps taken: {}\".format(time_steps))\n",
        "        print(\"Penalties incurred: {}\".format(penalties))\n",
        "    # Return the frame data, the total penalties, and the total time steps\n",
        "    return frames, penalties, time_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5lkc3Q5XLv"
      },
      "source": [
        "With the baseline approach defined, we will run a test with this approach to see how long it takes an agent using this approach to find a solution for simulation 328 and how many major penalties the agent receives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTLKSCRq4bEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d239ae8-8f81-4517-98ad-8d2d9628ceaa"
      },
      "source": [
        "state = 328\n",
        "# Run a test and collect all frames from the run\n",
        "frames, _, _ = run_single_simulation_baseline(env, state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for simulation: 328\n",
            "Timesteps taken: 3597\n",
            "Penalties incurred: 1174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayN1up479Fl1"
      },
      "source": [
        "After performing a simulation and retrieving the results, we can use the frames obtained from the simulation and pass it to the *print_frames* function below to display an animation containing all frames along with the information within that frame at each timestep.    \n",
        "\n",
        "For the first episode that you view, it is recommended to run through the entire process at a slower speed (such as 0.3 or 0.5 for the sleepTime parameter). However you are free to increase the speed of the process later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt-sMubValJt"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames, sleepTime=0.3):\n",
        "    '''\n",
        "    For each frame, show the frame and display the timestep it occurred at,\n",
        "    the number of the active state, the action selected, adn the corresponding reward.\n",
        "    '''\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        # Can adjust speed here\n",
        "        sleep(sleepTime)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsgBOQI3MpG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e5ffc7-4f24-42ba-a035-375f6eba8f73"
      },
      "source": [
        "# Print the frames from the episode\n",
        "print_frames(frames, 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 3597\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRCoIPbi7OCv"
      },
      "source": [
        "**(TO DO) Q2 - 2 marks**   \n",
        "Let's look at a few simulations to see how the random approach behaves.  To do so, we'll start from the states you've defined in Q1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-N9fP_Ydvot"
      },
      "source": [
        "**(TO DO) Q2 (a) - 1 mark**   \n",
        "Using the state defined from Q1 (a), retrieve the corresponding frames obtained from using the baseline approach defined above. Then display those frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4IqQE634Lm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6736b76a-e7c7-499d-9643-9ae201112c4e"
      },
      "source": [
        "# ANSWER Q2(a)\n",
        "# Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (a). Then show those frames.\n",
        "frames_1a, _, _ = run_single_simulation_baseline(env, state_1a)\n",
        "print_frames(frames_1a, 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 8676\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Dufixud28j"
      },
      "source": [
        "**(TO DO) Q2 (b) - 1 mark**   \n",
        "Using the state defined from Q1 (b), retrieve the corresponding frames obtained from using the baseline approach defined above. Then display those frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfEO2-zZ4To9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f322b135-c666-4af8-c99f-dda5e5d87e38"
      },
      "source": [
        "# ANSWER Q2(b)\n",
        "# Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (a). Then show those frames.\n",
        "frames_1b, _, _ = run_single_simulation_baseline(env, state_1b)\n",
        "print_frames(frames_1b, 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 22\n",
            "State: 475\n",
            "Action: 5\n",
            "Reward: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwgxcH_Ojfud"
      },
      "source": [
        "With the ability to simulate single runs of an episode with the Baseline approach, we will now define a function that we will use to evaluate the general performance of the baseline model when averaged over many episodes. The *evaluate_agent_baseline* function below accepts as input the total number of randomly selected episodes to run along with the environment, runs the random episodes, displays the average amount of timesteps taken per episode along with the average penalties incurred, and returns the frame data.     \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLAT5_mjgEu"
      },
      "source": [
        "def evaluate_agent_baseline(episodes, env):\n",
        "    '''\n",
        "    Given a number of episodes and an environment, run the specified\n",
        "    number of episodes, where each run begins with a random state, display the\n",
        "    naverage timesteps per episode and the average penalties per episode, and output\n",
        "    the frames to be displayed.\n",
        "    '''\n",
        "    total_time_steps, total_penalties = 0, 0\n",
        "    frames = []\n",
        "    # Run through the total number of episodes\n",
        "    for _ in range(episodes):\n",
        "        # Get a random state\n",
        "        state = env.reset()\n",
        "        # Run the simulation, obtaining the results\n",
        "        frame_data, penalties, time_steps = run_single_simulation_baseline(env, state, True)\n",
        "        # Update the tracked data over all simulations\n",
        "        total_penalties += penalties\n",
        "        total_time_steps += time_steps\n",
        "        frames = frames + frame_data\n",
        "    print(f\"Results after {episodes} episodes:\")\n",
        "    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n",
        "    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "    return frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHkepDd2C3im"
      },
      "source": [
        "**(TO DO) Q3 - 5 marks**    \n",
        "Let's evaluate the baseline approach.  We must run a certain number of episodes to average the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Tnw6bBxMXv"
      },
      "source": [
        "**(TO DO) Q3 (a) - 1 mark**    \n",
        "Use the *evaluate_agent_baseline* function defined above to run through 100 random episodes for the environment.   \n",
        "\n",
        "***If the evaluate_agent_baseline function ever seems to be running for far too long (several minutes, not just one), stop the run by clicking the button at the top-left of the code cell being executed and run it again.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1BQOaSu4awd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4349af88-c95d-46e4-a594-cce212fce577"
      },
      "source": [
        "# ANSWER Q3(a)\n",
        "frames_3a = evaluate_agent_baseline(100, env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 2249.37\n",
            "Average penalties per episode: 726.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dClJX6AWxZM5"
      },
      "source": [
        "**(TO DO) Q3 (b) - 2 marks**    \n",
        "From the output seen from Q3 (a), how did the Baseline approach do and why do you think that it performed well or poorly? Explain with respect to the average timesteps per episode and the average penalties per episode.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWVDzBDc3djW"
      },
      "source": [
        "**ANSWER Q3(b)**   \n",
        "\n",
        "The Baseline approach performed poorly because the average timesteps per episode was 2249.37, and the average penalties per episode was 726.94. These are indicators of poor performance because that means it took a large number of timesteps per episode to get to the destination, which is not the desired outcome (to minimize the number of steps). Furthermore, the desired outcomes for penalties is zero or a value close to zero; thus, the large value for the average number of penalties from the simulation supports the fact that the Baseline approach performed poorly.  \n",
        "\n",
        "I think the Baseline approach performed poorly because of the random nature of the algorithm, which selects a random available action at each step. So, a kind of brute force approach was used, which is not always the most optimal method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wr3F6IExrQQ"
      },
      "source": [
        "**(TO DO) Q3 (c) - 2 marks**    \n",
        "Without suggesting to use a Reinforcment Learning approach (as we would do next), suggest 2 ideas that could be included in a Baseline+ approach to allow it to perform slightly better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQRkm9-I4j2G"
      },
      "source": [
        "**ANSWER Q3(c)**\n",
        "\n",
        "Two other ideas that could help improve the Baseline approach would be a greedy strategy, with a (1) random restart algorithm or a (2) random modification algorithm. A greedy strategy is not the best solution since it tends to find a locally optimal solution. However, implementing that with a random restart can bring it closer to finding a globally optimal solution. The same applies for a random modification algorithm, since jumping to random places might lead to exploring a space toward a globally optimal solution. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GfrTVl3AuhD"
      },
      "source": [
        "**3.0 - Training an Agent with Q-Learning to play the Taxi Game**   \n",
        "\n",
        "Now that we have seen simulations of the autonomous taxi using a random strategy, we will use Q-Learning to try applying a Reinforcement Learning approach to the problem and have the autonomous taxi learn a better strategy.\n",
        "\n",
        "To start the process, we will create a table of Q values for each action-state possibility (initializing it as zero). The agent will update this table when training and ATTENTION will need to ***reinitialize*** the table whenever the agent wants to restart its training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDTbzsaEanoU"
      },
      "source": [
        "# Initialize the table of Q values for the state-action pairs\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mph6_GdSGy2F"
      },
      "source": [
        "With the Q-table initialized, we will now define the training function that adjusts the Q values within *q_table*. The training process consists of running through a number of random simulations and updating the Q values for each state via Q-Learning.    \n",
        "\n",
        "There are a number of hyperparameters used by the training function:    \n",
        "\n",
        "- *alpha*: Learning parameter \n",
        "- *gamma*: The long term reward discount parameter.    \n",
        "- *epsilon*: Exploitation/Exploration parameter \n",
        "- *num_simulations*: Represents how many episodes should be generated for the autonomous taxi update the Q-values.\n",
        "\n",
        "Thus, by running through this learning algorithm, an agent can learn which Q-values to use when later put in a test situation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kbLhT-Ua0Cc"
      },
      "source": [
        "def train_agent(alpha, gamma, epsilon, num_simulations):\n",
        "    '''\n",
        "    Trains an agent by updating its Q values for a total of num_simulations\n",
        "    episodes with the alpha, gamma, and epsilon hyperparameters. \n",
        "    '''\n",
        "    # For plotting metrics\n",
        "    all_time_steps = []\n",
        "    all_penalties = []\n",
        "    # Generate the specified number of episodes\n",
        "    for i in range(1, num_simulations + 1):\n",
        "        # Generate a new state by resetting it\n",
        "        state = env.reset()\n",
        "        # Variables tracked (time steps, total penalties, the reward value)\n",
        "        time_steps, penalties, reward, = 0, 0, 0\n",
        "        done = False\n",
        "        # Run the simulation \n",
        "        while not done:\n",
        "            # Select a random action is the randomly selected number from a\n",
        "            # uniform distribution is less than epsilon\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = env.action_space.sample() # Explore action space\n",
        "            # Otherwise use the currently learned Q values\n",
        "            else:\n",
        "                action = np.argmax(q_table[state]) # Exploit learned values\n",
        "            # Retrieve the relevant information after performing the action\n",
        "            next_state, reward, done, info = env.step(action) \n",
        "            # Retrieve the old Q value and the maximum Q value from the next state\n",
        "            old_value = q_table[state, action]\n",
        "            next_max = np.max(q_table[next_state])\n",
        "            # Update the current Q value\n",
        "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "            q_table[state, action] = new_value\n",
        "            # Track anytime an incorrect dropoff or pickup is made\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "            # Proceed to the next state and time step\n",
        "            state = next_state\n",
        "            time_steps += 1\n",
        "        # Display progress for each 100 episodes\n",
        "        if i % 100 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Episode: {i}\")\n",
        "    print(\"Training finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLoNggkqQu2L"
      },
      "source": [
        "We now use the training function with a set of hyperparameters to train the agent with Q-Learning to potentially improve performance over time.  The number of episodes is set to 100000, so the learning will take a bit of time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tkJvaTwPjfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1afa55c6-14b2-403a-d23f-cf0e2ac30993"
      },
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.5\n",
        "epsilon = 0.1\n",
        "num_simulations = 100000\n",
        "# Train the agent\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmzQceEvRyT3"
      },
      "source": [
        "After the training, we can look at the Q-values that have been obtained in our state-action table for a specific state. Below we see that each Q-value for the six possible actions available for state 328 have been updated accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvXOzHtVAar6"
      },
      "source": [
        "**(TO DO) Q4 - 2 marks**     \n",
        "Below we print the Q-values that are available for the six actions at state 328 and we render that state to view it. Based on the available Q-values (assuming we are in exploitation mode), which action would be the next to be selected (or if there are ties, list all possible actions that would be considered)? Do any of the actions that contain larger Q values seem problematic if they were selected? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KP5FMBQa1el",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a90494-e18f-4baa-b44d-555131c60d8c"
      },
      "source": [
        "print(q_table[328])\n",
        "env.s = 328\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -1.98659605  -1.95703125  -1.98411132  -1.97592292 -10.25457278\n",
            " -10.50485063]\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ms4hY68Fpg"
      },
      "source": [
        "**ANSWER Q4**\n",
        "\n",
        "Assuming we are in exploitation mode, based on the available Q values, the action that would be selected next is *move north* (labelled as 1) with a Q value of -1.95703125.  \n",
        "  \n",
        "The problematic actions are pickup passenger (labelled as 4) with a Q value of -10.25457278, and drop off passenger (labelled as 5) with a Q value of -10.50485063. Because both these actions have very large Q values that far exceed the other actions, then the long-term reward would be poorly affected, so it would be quite problematic to choose either.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncwGTV5PSHZx"
      },
      "source": [
        "With the training complete, we can now evaluate the Q-Learning approach in a similar method that we used to evaluate the Baseline approach. By passing the number of episodes to test for and the environment, we generate that number of random episodes and average the results obtained from running the Q-Learning approach to complete the episodes. Unlike the training, it is important to note that the hyperparameters that were used for learning are not used here. The agent simply uses the maximum Q-value at each step to determine which action to take at a given time step. This means we use a greedy policy (not even an epsilon-greedy one) which always trust the maximum Q-value to decide on its action.  It's a full exploitation mode (not exploration/exploitation).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuitaI8Ra2u8"
      },
      "source": [
        "def evaluate_agent_QL(episodes, env):\n",
        "    '''\n",
        "    Given a number to specify how many random states to run and the environment to use,\n",
        "    display the averaged metrics obtained from the tests and return the frames obtained from the tests.\n",
        "    '''\n",
        "    total_time_steps, total_penalties = 0, 0\n",
        "    frames = []\n",
        "    for _ in range(episodes):\n",
        "        # Generate a random state to use\n",
        "        state = env.reset()\n",
        "        # The information collected throughout the run\n",
        "        time_steps, penalties, reward = 0, 0, 0\n",
        "        # Determines when the episode is complete\n",
        "        done = False\n",
        "        # Run through the episode until complete\n",
        "        while not done:\n",
        "            # Select the action containing the maximum Q value\n",
        "            action = np.argmax(q_table[state])\n",
        "            # Run that action and retrieve the reward and other details\n",
        "            state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Put each rendered frame into dict for animation\n",
        "            frames.append({\n",
        "                'frame': env.render(mode='ansi'),\n",
        "                'state': state,\n",
        "                'action': action,\n",
        "                'reward': reward\n",
        "                }\n",
        "            )\n",
        "            # Specify whether the agent incorrectly chose to pick up or dropoff a passenger\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "            # Increment the current time step\n",
        "            time_steps += 1\n",
        "        # Track the totals\n",
        "        total_penalties += penalties\n",
        "        total_time_steps += time_steps\n",
        "    # Display the performance over the tests\n",
        "    print(f\"Results after {episodes} episodes:\")\n",
        "    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n",
        "    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "    # Return the frames to allow a user to view the runs\n",
        "    return frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEQNpWoSF822"
      },
      "source": [
        "**(TO DO) Q5 - 3 marks**     \n",
        " There is a random aspect in Q-learning, so we need to run for a certain number of episodes to provide an average result. Let's set that number high enough for the average to be significant. Let's see how Q-learning does compared to the baseline approach.\n",
        "\n",
        " ***ATTENTION: If the evaluate_agent_QL function ever seems to be running for far too long (30s or more), stop the run by clicking the button at the top-left of the code cell being executed and run it again. This occurs because the training was insufficient at setting valid Q values and resulted in a dead-end for a specific state.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TzyRekwGolK"
      },
      "source": [
        "**(TO DO) Q5 (a) - 1 mark**     \n",
        "Run the *evaluate_agent_QL* for 500 episodes to retrieve the average number of time steps and the average penalty after training.     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATGdih3m-TE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3275fabe-593c-4c22-c23f-211c6fc180c9"
      },
      "source": [
        "# ANSWER Q5(a)\n",
        "frames_5a = evaluate_agent_QL(500, env)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 500 episodes:\n",
            "Average timesteps per episode: 13.126\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Os_s3UGxK2"
      },
      "source": [
        "**(TO DO) Q5 (b) - 2 marks**     \n",
        "Given your results from Q5 (a), how do the observed results from the tests compare to the tests from the Baseline model in Q3 (a)? Specifically, which policy performs better with respect to the average number of penalties throughout the tests and which strategy is able to take passengers from start to destination more rapidly (on average).     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxE9w0Zc-Ve9"
      },
      "source": [
        "**ANSWER Q5(b)**  \n",
        "When comparing the observed results from the tests, it can be shown that the policy for Q5(a) performed significantly better than the policy for the Baseline model in Q3(a). For Q3(a), the average timesteps per episode is 2249.37, and for Q5(a), the average timesteps per episode is 13.126, which is a difference of 2236.244. Thus, Q5(a) has a more performant policy because it is able to take passengers from start to destination much more rapidly than the Baseline model in Q3(a). Furthermore, for Q3(a), the average penalties per episode is 726.94, and for Q5(a), the average penalties per episode is 0.0, which means the policy in Q5(a) performs better with respect to the average number of penalties (because its value is essentially 0).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woAhj7vqaD5q"
      },
      "source": [
        "**4.0 - Testing Different Hyperparameters**   \n",
        "\n",
        "Now we will try retraining the agent using different set ups for the hyperparameters. This will allow you to explore their impact on the Q-Learning as well as understand their purpose during the training.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBamDNUIZ0Iy"
      },
      "source": [
        "**(TO DO) Q6 - 12 marks**     \n",
        "Below we explore variations for all four hyperparameters used by the Q-Learning approach to better understand their impact on the training. When answering the questions, ***be careful to correctly set the hyperparameters***.         \n",
        "\n",
        "As a note, below are the initial hyperparameter values used from section 3.0 of this notebook to use as reference:    \n",
        "\n",
        "*alpha* = 0.1   \n",
        "*gamma* = 0.5   \n",
        "*epsilon* = 0.1   \n",
        "*num_simulations* = 100000 \n",
        "\n",
        "For all our evaluations, we will use 500 episodes.  Also, after each experiment, you will print the qtable value for state 328, so you can compare it with what was obtained in Q4 earlier.\n",
        "\n",
        "***ATTENTION (repeated from Q5): If the evaluate_agent_QL function ever seems to be running for far too long (30s or more), stop the run by clicking the button at the top-left of the code cell being executed and run it again. This occurs because the training was insufficient at setting valid Q values and resulted in a dead-end for a specific state.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQxxvn0xaxrC"
      },
      "source": [
        "**(TO DO) Q6 (a) - 1.5 marks**     \n",
        "Retrain the agent by resetting the Q learning values and training for only **35000 episodes** (with the same alpha, gamma, and epsilon values used in section 3.0 of this notebook). Then perform another test for 500 episodes with the environment.  Print the *q_table* for state 328."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgprtPOpJcUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d037432-b286-4c7e-af10-0d5ba25e2ff0"
      },
      "source": [
        "# ANSWER Q6(a)\n",
        "# TODO: Reset q_table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "# TODO: Retrain with the specified hyperparameters\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.5\n",
        "epsilon = 0.1\n",
        "num_simulations = 35000\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)\n",
        "# TODO: Test for 500 episodes\n",
        "frames_6a = evaluate_agent_QL(500, env)\n",
        "# TODO: Print q_table for state 328\n",
        "print(q_table[328])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 35000\n",
            "Training finished.\n",
            "\n",
            "Results after 500 episodes:\n",
            "Average timesteps per episode: 13.148\n",
            "Average penalties per episode: 0.0\n",
            "[-1.97283402 -1.95703125 -1.97805076 -1.96125526 -8.88292347 -8.65427996]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kSCmM6VFdHu"
      },
      "source": [
        "**(TO DO) Q6 (b) - 1.5 marks**      \n",
        "Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **epsilon value of 0.8** (with the same alpha and gamma values used in section 3.0 of this notebook). Then perform another test for 500 episodes with the environment.  Print the *q_table* for state 328."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s55VwNKJJzn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e06f5bd-f9b0-4c0f-d59c-1a471b9385f7"
      },
      "source": [
        "# ANSWER Q6(b)\n",
        "# TODO: Reset q_table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "# TODO: Retrain with the specified hyperparameters\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.5\n",
        "epsilon = 0.8\n",
        "num_simulations = 100000\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)\n",
        "# TODO: Test for 500 episodes\n",
        "frames_6b = evaluate_agent_QL(500, env)\n",
        "# TODO: Print q_table for state 328\n",
        "print(q_table[328])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "Results after 500 episodes:\n",
            "Average timesteps per episode: 13.24\n",
            "Average penalties per episode: 0.0\n",
            "[ -1.98925781  -1.95703125  -1.98925781  -1.97851563 -10.97851563\n",
            " -10.97851563]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcn_doxFm4H"
      },
      "source": [
        "**(TO DO) Q6 (c) - 1.5 marks**      \n",
        "Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **alpha value of 0.7** (with the same gamma and epsilon values used in section 3.0 of this notebook). Then perform another test for 500 episodes with the environment.    Print the *q_table* for state 328. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNwWugyUJ3VZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42fc53b8-b1d4-4c4d-e44f-9b566886eaac"
      },
      "source": [
        "# ANSWER Q6(c)\n",
        "# TODO: Reset q_table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "# TODO: Retrain with the specified hyperparameters\n",
        "# Hyperparameters\n",
        "alpha = 0.7\n",
        "gamma = 0.5\n",
        "epsilon = 0.1\n",
        "num_simulations = 100000\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)\n",
        "# TODO: Test for 500 episodes\n",
        "frames_6c = evaluate_agent_QL(500, env)\n",
        "# TODO: Print q_table for state 328\n",
        "print(q_table[328])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "Results after 500 episodes:\n",
            "Average timesteps per episode: 13.114\n",
            "Average penalties per episode: 0.0\n",
            "[ -1.98925781  -1.95703125  -1.98925781  -1.97851562 -10.97851562\n",
            " -10.97851562]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZwH8XogFreB"
      },
      "source": [
        "**(TO DO) Q6 (d) - 1.5 marks**      \n",
        "Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **gamma value of 0.15** (with the same alpha and epsilon values used in section 3.0 of this notebook). Then perform another test for 500 episodes with the environment.  Print the *q_table* for state 328.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPS0kF_YJ51X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9f88fd-2e1e-4f62-a107-e82358b241ea"
      },
      "source": [
        "# ANSWER Q6(d)\n",
        "# TODO: Reset q_table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "# TODO: Retrain with the specified hyperparameters\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.15\n",
        "epsilon = 0.1\n",
        "num_simulations = 100000\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)\n",
        "# TODO: Test for 500 episodes\n",
        "frames_6d = evaluate_agent_QL(500, env)\n",
        "# TODO: Print q_table for state 328\n",
        "print(q_table[328])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "Results after 500 episodes:\n",
            "Average timesteps per episode: 13.346\n",
            "Average penalties per episode: 0.0\n",
            "[-1.17647045 -1.17646977 -1.17647049 -1.17647037 -9.94596458 -9.99010609]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec4qnmyJbWYw"
      },
      "source": [
        "**(TO DO) Q6 (e) - 6 marks**      \n",
        "Using the results obtained from your tests in Q6 (a), (b), (c), and (d), along with the initial results found from Q5 to serve as the Q-Learning baseline to compare with, explain the impacts of modifying the number of episodes trained on (less vs more), the alpha value (lower vs higher), the gamma value (lower vs higher), and the epsilon value (lower vs higher).  Even if the difference in the comparisons are minor, state them.  \n",
        "\n",
        "Besides looking at the obtained results (average number of timesteps), discuss the impact on the q-values for the different actions in state 328. Did those change?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h6XkCsJKnGP"
      },
      "source": [
        "**ANSWER Q6(e)**  \n",
        "\n",
        "Make sure you discuss each hyperparameter in relation to timesteps and qvalues  \n",
        "compare with, explain the impacts of modifying  \n",
        "\n",
        "**Number of Episodes trained on: Less vs. More**  \n",
        "To compare the number of episodes trained on, we will look at the results obtained from Q5 and the results obtained from Q6(a) because the only major change between these two models is the number of episodes trained on. Q5 trained on 100000 episodes and Q6(a) trained on 35000 episodes, which is a difference of 65000. The impact of reducing the number of episodes trained seemed to produce a slightly less optimal result for the average timesteps per episode. The average timesteps per episode for Q5 is 13.126, but for Q6(a), it is 13.148; thus, in this case, a reduction in 65000 episodes trained on has increased the average timesteps per episode by 0.022. Since we want a small number of timesteps, it appears that having less episodes trained on will reduce the performance in terms of average timesteps per episode (though by a small margin). Thus, it is better to have more episodes trained on if we want to improve performance.  \n",
        "\n",
        "To make things easier to visualize, here are the Q-Values for:   \n",
        "Q5: [-1.98659605, -1.95703125, -1.98411132, -1.97592292, -10.25457278, -10.50485063]  \n",
        "Q6(a): [-1.97283402, -1.95703125, -1.97805076, -1.96125526, -8.88292347, -8.65427996]   \n",
        "\n",
        "When looking at the Q-Values, it seems that the reduction of the *number of episodes trained on* has an impact for state 328, but what is not affected is the action to be selected next. In other words, both the policy in Q5 and Q6a will choose *move north* as the next action for state 328. However, for state 328, most of the Q-values for 6(a) appear to be slightly larger than those for Q5, apart from the Q-value for *move up* (at index 1), which is identical for both. The largest difference appears at index 4 and index 5, for actions *pickup passenger* and *drop off passenger*, respectively. For Q6(a), index 4 increased to -8.88292347 from -10.25457278 in Q5, which is a difference of approximately 1.37, and index 5 increased by approximately 1.85. Other increases were very minimal. For example, the q-values at index 0 (*move south*), index 2 (*move east*), and index 3 (*move west*) did not increase by more than approximately 0.015.\n",
        "  \n",
        "\n",
        "**Epsilon Value: Lower vs Higher**  \n",
        "To analyze the epsilon value, we will compare the results from Q5 to the results in Q6(b) because the only notable change between these two models is the epsilon value. Q5 has an epsilon value of 0.1 and Q6(b) has an epsilon value of 0.8, which is a difference of 0.7. It appears that increasing the epsilon value will poorly impact the performance for the average timesteps per episode. The results support this claim because the average timesteps per episode for Q6(b) is 13.24 and for Q5 is 13.126, that means increasing the epsilon value by 0.7 has increased the average timesteps per episode by 0.114 (in this context). Thus, a higher epsilon value will yield a higher value for average timesteps per episode, which is a decrease in performance. Conversely, a lower epsilon value will yield a lower average timesteps per episode, which means an increase in performance.  \n",
        "\n",
        "To make things easier to visualize, here are the Q-Values for:   \n",
        "Q5: [-1.98659605, -1.95703125, -1.98411132, -1.97592292, -10.25457278, -10.50485063]\n",
        "Q6(b): [-1.98925781, -1.95703125, -1.98925781, -1.97851563, -10.97851563, -10.97851563]  \n",
        "\n",
        "When looking at the Q-Values, it seems that increasing the epsilon value will have an impact for state 328. All the Q-Values in Q6(b) seem to have a very small decrease from those in Q5, except for *move north*, which has an identical Q-value of -1.95703125 for both Q5 and Q6(b). It is interesting to note that the action to be selected next is not affected; both Q5 and Q6(b) will select *move north* as the next action for state 328. The majority of decreases from Q5 to Q6(b) were very small and less than approximately 0.01. The largest of these decreases were seen at index 4 (*pickup passenger*) from -10.25457278 in Q5 to  -10.97851563 in Q6(b), which is a difference of approximately 0.724. No other actions for state 328 surpassed this q-value when we refer to the increase.\n",
        "\n",
        "  \n",
        "**Alpha Value: Lower vs Higher**  \n",
        "To analyze the alpha value, we will look at the results taken from Q5 and compare those to the results from Q6(c) because the only significant modification between these two models is the alpha value. Q5 has an alpha value of 0.1 and Q6(c) has an alpha value of 0.7, which is a difference of 0.6. The average timesteps per episode for Q5 is 13.126 and the average timesteps per episode for Q6(c) is 13.114, which is a difference of 0.012. For this situation, we can see that an increase in the alpha value by 0.6 has lead to a decrease in the average timesteps per episode by 0.012, which is desirable since we want to minimize the average number of timesteps. Thus, based on the results, we can state that a higher alpha value improves performance because it reduces the average timesteps per episode. We can also state that a lower alpha value will lead to worse performance because it will increase the average timesteps per episode.  \n",
        "  \n",
        "To make things easier to visualize, here are the Q-Values for:   \n",
        "Q5: [-1.98659605, -1.95703125, -1.98411132, -1.97592292, -10.25457278, -10.50485063]  \n",
        "Q6(c): [ -1.98925781  -1.95703125  -1.98925781  -1.97851562 -10.97851562\n",
        " -10.97851562]   \n",
        "  \n",
        "When looking at the Q-Values, it seems that increasing the alpha value will have an impact for state 328. All the Q-Values in Q6(c) seem to have a very small decrease from those in Q5, except for *move north*, which has an identical Q-value of -1.95703125 for both Q5 and Q6(c). Once again, it is interesting to see that the action to be selected next is not affected; both Q5 and Q6(c) will select *move north* as the next action for state 328. Once again, the largest differences can be seen at index 4 and 5, for the actions *pickup passenger* and *drop off passenger*, respectively.\n",
        "\n",
        "\n",
        "**Gamma Value: Lower vs Higher**  \n",
        "To analyze the gamma value, we will compare the results from Q5 to those in Q6(d) because the only notable change between these two models is the gamma value. Q5 has an gamma value of 0.5 and Q6(d) has an gamma value of 0.15, which is a difference of 0.35. The average timesteps per episode for Q5 is 13.126 and the average timesteps per episode for Q6(d) is 13.346, which is a difference of 0.22. In this context, the results show that decreasing the gamma value by 0.35 has increased the average timesteps per episode by 0.22. Thus, based on the results, a lower gamma value equates a higher average timesteps per episode, which is a reduction in performance. Conversely, we can say that a higher gamma value will improve performance, since it will decrease the average timesteps per episode.  \n",
        "  \n",
        "To make things easier to visualize, here are the Q-Values for:   \n",
        "Q5: [-1.98659605, -1.95703125, -1.98411132, -1.97592292, -10.25457278, -10.50485063]  \n",
        "Q6(d): [-1.17647045 -1.17646977 -1.17647049 -1.17647037 -9.94596458 -9.99010609]  \n",
        "\n",
        "When looking at the Q-Values for state 328, it seems that decreasing (or adjusting) the gamma value has the largest impact than adjusting any of the other values (alpha, epsilon) or the *number of episodes trained on*. Decreasing the gamma value is the only time where we see a difference for the action *move north* from Q5 to Q6(d). Although move north* is still selected as next action, we see a change in its Q-value for the first time, with an increase of approximately 0.781 from Q5 to Q6(d). The rest of the differences for the Q-values approximate around this value. However, index 4 (*pickup passenger*) and index 5 (*drop off passenger*) show the largest differences again, at approximately 0.31 and 0.52, respectively.  \n",
        "\n",
        "All in all, I also want to state that I did not place much emphasis for *the average penalties per episode* in my comparisons because they all had the same value of 0.0 across all tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z7LljGSQ5bI"
      },
      "source": [
        "**5.0 - Scaling up the environment**   \n",
        "\n",
        "As we've seen in the video lectures, and as you've experienced here, a limitation of Q-Learning is that Q-Values must be learned for all state/action pairs.  In our toy problem of the autonomous taxi, we worked with a small grid (5x5) and only 4 pick-up/drop-off locations, and we were already at 500 states, so with 6 actions, that meant 3000 state/action pairs, so 3000 q-values to learn.\n",
        "\n",
        "Imagine a larger grid of 100x100, and imagine all grid cells could be pick-up or drop-off cells, then Q-learning would not be a realistic approach and we would have to go to Deep Q-Learning, as we saw in the course videos.\n",
        "\n",
        "Implementing a Deep Q-Learning approach is beyond the scope of this notebook, but we can still think about the type of architecture it would require."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FeWLLRhRYKX"
      },
      "source": [
        "**(TO DO) Q7 - 2 marks**      \n",
        "Given this new description of a larger grid, and a larger set of pick-up/drop-off locations, describe what a deep Q-learning neural network could look like.  What could be put at the input nodes and how many would be required?  What would we try to learn at the output nodes and how many output nodes would be required.?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VAzlr6URY5f"
      },
      "source": [
        "**ANSWER Q7**\n",
        "\n",
        "The input nodes could be all the possible states.\n",
        "From the output nodes, we would try to learn the best action given a state.\n",
        "We would have as many output nodes as we have actions. So we would have 6 output nodes, a node for *move south*, *move north*, *move east*, *move west*, *pickup passenger*, and *drop off passenger*. "
      ]
    }
  ]
}